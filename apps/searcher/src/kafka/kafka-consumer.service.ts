import { Injectable, Logger, OnModuleInit, OnModuleDestroy } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { Kafka, Consumer, EachMessagePayload, Producer, Partitioners } from 'kafkajs';

export interface KafkaMessage {
  topic: string;
  partition: number;
  offset: string;
  key: string | null;
  value: any;
  headers?: any;
  timestamp?: string;
}

export interface FailedMessage {
  originalTopic: string;
  message: KafkaMessage;
  error: string;
  attemptCount: number;
  firstAttemptAt: Date;
  lastAttemptAt: Date;
}

@Injectable()
export class KafkaConsumerService implements OnModuleInit, OnModuleDestroy {
  private kafka: Kafka;
  private consumer: Consumer;
  private producer: Producer;
  private readonly logger = new Logger(KafkaConsumerService.name);
  private messageHandlers = new Map<string, (message: KafkaMessage) => Promise<void>>();
  private readonly MAX_RETRIES = 3;
  private readonly DLQ_TOPIC = 'searcher.dlq';
  private isShuttingDown = false;

  constructor(private configService: ConfigService) {}

  async onModuleInit() {
    try {
      this.kafka = new Kafka({
        clientId: 'searcher-service',
        brokers: this.configService.get('KAFKA_BROKERS', 'localhost:9092').split(','),
        retry: {
          initialRetryTime: 300,
          retries: 8,
          maxRetryTime: 30000,
          multiplier: 2,
        },
        connectionTimeout: 30000,
        requestTimeout: 30000,
      });

      this.consumer = this.kafka.consumer({ 
        groupId: 'searcher-consumer-group',
        sessionTimeout: 30000,
        heartbeatInterval: 3000,
        maxWaitTimeInMs: 5000,
        retry: {
          retries: 5,
          initialRetryTime: 300,
          maxRetryTime: 30000,
        },
      });

      // Kh·ªüi t·∫°o producer cho DLQ
      this.producer = this.kafka.producer({
        createPartitioner: Partitioners.LegacyPartitioner,
        retry: {
          retries: 5,
          initialRetryTime: 300,
        },
      });

      await this.consumer.connect();
      await this.producer.connect();
      this.logger.log('‚úÖ Connected to Kafka (Consumer & Producer)');

      // Subscribe to topics
      await this.subscribeToTopics();
      
      // Start consuming
      await this.consumer.run({
        eachMessage: async (payload: EachMessagePayload) => {
          if (this.isShuttingDown) {
            this.logger.warn('‚ö†Ô∏è Skipping message processing - service is shutting down');
            return;
          }
          await this.handleMessage(payload);
        },
        // X·ª≠ l√Ω t·ª´ng message m·ªôt ƒë·ªÉ ƒë·∫£m b·∫£o order v√† retry logic
        partitionsConsumedConcurrently: 1,
      });

      this.logger.log('üöÄ Kafka consumer started with reliable message processing');
    } catch (error) {
      this.logger.error('‚ùå Failed to initialize Kafka consumer:', error);
      throw error;
    }
  }

  async onModuleDestroy() {
    this.isShuttingDown = true;
    this.logger.warn('üõë Starting graceful shutdown...');

    try {
      // ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ message hi·ªán t·∫°i ƒë∆∞·ª£c x·ª≠ l√Ω xong
      await new Promise(resolve => setTimeout(resolve, 5000));

      // Disconnect consumer v√† producer (offsets will be auto-committed)
      if (this.consumer) {
        await this.consumer.disconnect();
        this.logger.log('‚úÖ Kafka consumer disconnected');
      }

      if (this.producer) {
        await this.producer.disconnect();
        this.logger.log('‚úÖ Kafka producer disconnected');
      }
    } catch (error) {
      this.logger.error('‚ùå Error during graceful shutdown:', error);
    }
  }

  private async subscribeToTopics() {
    const topics = [
      'user.created',
      'user.updated',
      'user.deleted',
      'role.created',
      'role.updated',
      'role.deleted',
      'permission.created',
      'permission.updated',
      'permission.deleted',
    ];

    for (const topic of topics) {
      // fromBeginning: false nghƒ©a l√† ch·ªâ ƒë·ªçc message m·ªõi t·ª´ khi consumer connect
      // Nh∆∞ng nh·ªù consumer group, offset s·∫Ω ƒë∆∞·ª£c l∆∞u l·∫°i
      // Khi service restart, n√≥ s·∫Ω ti·∫øp t·ª•c t·ª´ offset cu·ªëi c√πng ƒë√£ commit
      await this.consumer.subscribe({ topic, fromBeginning: false });
      this.logger.log(`üì• Subscribed to topic: ${topic}`);
    }
  }

  private async handleMessage(payload: EachMessagePayload) {
    const { topic, partition, message } = payload;
    const attemptCount = this.getRetryAttempt(message);
    
    try {
      const kafkaMessage: KafkaMessage = {
        topic,
        partition,
        offset: message.offset,
        key: message.key ? message.key.toString() : null,
        value: message.value ? JSON.parse(message.value.toString()) : null,
        headers: message.headers,
        timestamp: message.timestamp,
      };

      this.logger.debug(`üì® Processing message from ${topic} (offset: ${message.offset}, attempt: ${attemptCount + 1})`);

      // Find and execute handler
      const handler = this.messageHandlers.get(topic);
      if (handler) {
        await handler(kafkaMessage);
        
        // Commit offset ch·ªâ khi x·ª≠ l√Ω th√†nh c√¥ng
        await this.consumer.commitOffsets([
          {
            topic,
            partition,
            offset: (BigInt(message.offset) + BigInt(1)).toString(),
          },
        ]);
        
        this.logger.debug(`‚úÖ Message processed and committed (offset: ${message.offset})`);
      } else {
        this.logger.warn(`‚ö†Ô∏è No handler found for topic: ${topic}`);
        // V·∫´n commit ƒë·ªÉ kh√¥ng block queue
        await this.consumer.commitOffsets([
          {
            topic,
            partition,
            offset: (BigInt(message.offset) + BigInt(1)).toString(),
          },
        ]);
      }
    } catch (error) {
      this.logger.error(`‚ùå Failed to process message from topic ${topic} (offset: ${message.offset}):`, error);
      
      // Retry logic
      if (attemptCount < this.MAX_RETRIES) {
        this.logger.warn(`üîÑ Retrying message (attempt ${attemptCount + 1}/${this.MAX_RETRIES})...`);
        
        // Th√™m delay tr∆∞·ªõc khi retry (exponential backoff)
        const delayMs = Math.min(1000 * Math.pow(2, attemptCount), 10000);
        await new Promise(resolve => setTimeout(resolve, delayMs));
        
        // Kh√¥ng commit offset, ƒë·ªÉ consumer t·ª± ƒë·ªông retry message n√†y
        // Nh∆∞ng ƒë·ªÉ tr√°nh infinite loop, ta s·∫Ω track retry count trong headers
        throw error; // Re-throw ƒë·ªÉ Kafka bi·∫øt message ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω
      } else {
        // ƒê√£ v∆∞·ª£t qu√° s·ªë l·∫ßn retry, g·ª≠i v√†o DLQ
        await this.sendToDeadLetterQueue(topic, message, error, attemptCount);
        
        // Commit offset ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω message ti·∫øp theo
        await this.consumer.commitOffsets([
          {
            topic,
            partition,
            offset: (BigInt(message.offset) + BigInt(1)).toString(),
          },
        ]);
      }
    }
  }

  private getRetryAttempt(message: any): number {
    const retryHeader = message.headers?.['x-retry-count'];
    if (retryHeader) {
      return parseInt(retryHeader.toString(), 10);
    }
    return 0;
  }

  private async sendToDeadLetterQueue(
    originalTopic: string,
    message: any,
    error: any,
    attemptCount: number,
  ) {
    try {
      const failedMessage: FailedMessage = {
        originalTopic,
        message: {
          topic: originalTopic,
          partition: message.partition,
          offset: message.offset,
          key: message.key ? message.key.toString() : null,
          value: message.value ? JSON.parse(message.value.toString()) : null,
          headers: message.headers,
          timestamp: message.timestamp,
        },
        error: error.message || String(error),
        attemptCount,
        firstAttemptAt: new Date(),
        lastAttemptAt: new Date(),
      };

      await this.producer.send({
        topic: this.DLQ_TOPIC,
        messages: [
          {
            key: message.key,
            value: JSON.stringify(failedMessage),
            headers: {
              'x-original-topic': originalTopic,
              'x-error': error.message || String(error),
              'x-retry-count': attemptCount.toString(),
            },
          },
        ],
      });

      this.logger.error(`üíÄ Message sent to DLQ (topic: ${originalTopic}, offset: ${message.offset})`);
    } catch (dlqError) {
      this.logger.error(`‚ùå Failed to send message to DLQ:`, dlqError);
    }
  }

  registerHandler(topic: string, handler: (message: KafkaMessage) => Promise<void>) {
    this.messageHandlers.set(topic, handler);
    this.logger.log(`Handler registered for topic: ${topic}`);
  }

  // Method to manually produce messages for testing
  async produceMessage(topic: string, message: any, key?: string) {
    const producer = this.kafka.producer();
    await producer.connect();
    
    try {
      await producer.send({
        topic,
        messages: [
          {
            key,
            value: JSON.stringify(message),
          },
        ],
      });
      this.logger.debug(`Message sent to topic ${topic}`);
    } finally {
      await producer.disconnect();
    }
  }
}